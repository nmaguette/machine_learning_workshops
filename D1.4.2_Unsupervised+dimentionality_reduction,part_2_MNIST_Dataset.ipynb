{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Maguette_D1.4.2_Unsupervised+dimentionality reduction, part 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nmaguette/machine_learning_workshops/blob/master/D1.4.2_Unsupervised%2Bdimentionality_reduction%2Cpart_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qusOrgK_eD95",
        "colab_type": "text"
      },
      "source": [
        "# Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LbspxxleD9_",
        "colab_type": "text"
      },
      "source": [
        "$\\textbf{Introdution.}$ We have seen that the second step of statistics may be data description. It may be intricately linked to data cleaning: for instance, you realize your data needs cleaning when describing it through a histogram and ending up with absurd values.\n",
        "\n",
        "On the other hand, dimensionality reduction may serve different purposes:\n",
        "    \n",
        "- $\\textbf{Visualization:}$ if your initial data lies in a high dimensional space, you may want to visualize it\n",
        "in $\\mathbb{R}^2$ or $\\mathbb{R}^3$ or to detect possible patterns or anomalies;\n",
        "\n",
        "- $\\textbf{Providing a suitable input:}$ if your database is so large that you cannot practically compute\n",
        "estimates with it all, dimensionality reduction is mandatory;\n",
        "\n",
        "- $\\textbf{Avoiding the curse of dimensionality:}$ this expression refers to range of issues encountered\n",
        "when dealing with data in high-dimensional spaces. Indeed, it becomes more difficult to\n",
        "detect patterns overall, as data density decreases and computational complexity increases.\n",
        "Therefore dimensionality reduction techniques also help providing better suited inputs for\n",
        "models.\n",
        "\n",
        "$\\textbf{Dataset.}$ In this session, we are first focusing on a standard dataset in machine learning: the MNIST dataset of handwritten digits. It contains 70,000 black and white $28\\times 28$ pixel images representing handwritten digits from 0 to 9. We are going to answer the following questions:\n",
        "- How to visualize MNIST images in $\\mathbb{R}^2$ in a pertinent way (ie in way that a reflects intrinsic similarities) ?\n",
        "- How to assess whether dimensionality reduction has yielded qualitatively suitable outputs ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnLsh-xreD-E",
        "colab_type": "text"
      },
      "source": [
        "### Loading useful packages and the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1eKY7seE75x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install git+https://github.com/yhat/ggplot.git\n",
        "!pip install pandas==0.19.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DlDQ6AN6DuRu",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "#from ggplot import *\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Yw5BCTgvEt7R",
        "colab": {}
      },
      "source": [
        "# Load training and eval data\n",
        "((X, Y),\n",
        " (eval_data, eval_labels))=tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "X=X/np.float32(255)\n",
        "Y=Y.astype(np.int32)  # not required\n",
        "\n",
        "eval_data=eval_data/np.float32(255)\n",
        "eval_labels=eval_labels.astype(np.int32)  # not required\n",
        "\n",
        "print('Training data contains '+str(X.shape[0])+' examples of size '+str(X.shape[1])+'*'+str(X.shape[2]))\n",
        "print('Test data contains '+str(eval_data.shape[0])+' examples of size '+str(eval_data.shape[1])+'*'+str(eval_data.shape[2]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMZPJLe2eD-r",
        "colab_type": "text"
      },
      "source": [
        "### 1) Visualizing data examples: show how data look like, choose one example and show it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2L8l4LpJzkj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krksGUtReD-y",
        "colab_type": "text"
      },
      "source": [
        "We turn the database into a dataframe, which is a convenient format to manipulate large amounts of data. Each column corresponds to a pixel value, the last column being the image label (within $\\{0,..,9\\}$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-E2nMQ6XJ47j",
        "colab_type": "text"
      },
      "source": [
        "### 2) Turn data into dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zc3fVr7OKh8z",
        "colab": {}
      },
      "source": [
        "np.shape(X)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN3YJxqaeD-9",
        "colab_type": "text"
      },
      "source": [
        "### 3) Using this format, we can visualize a subsample of the digits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ShzPHfpnLgjh",
        "colab": {}
      },
      "source": [
        "# Permutation for random subsampling\n",
        "# Plot some chosen examples\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGUN9PdIeD_H",
        "colab_type": "text"
      },
      "source": [
        "### Dimensionality Reduction through Principal Component Analysis (PCA).\n",
        "\n",
        "The aim of PCA is to turn a set of possibly correlated variables into linearly uncorrelated variables (called “principal components”) through an orthogonal transformation. The idea is that:\n",
        "- the first component ($C_1$) accounts for most of the variance in the data;\n",
        "- the second ($C_2$) accounts for the second highest variance, and is orthogonal to $C_1$;\n",
        "- etc.\n",
        "The orthogonality condition makes the data way easier to visualize, while the variance criterion ranks components by order of relative importance.\n",
        "\n",
        "In order to achieve that, the optimization procedure aims at finding a matrix of weights $W$ mapping each $x_i \\in \\mathbb{R}^d$ to a principal components score $t_{k,i}=\\langle x_i, w_k\\rangle$ inheriting the most variance from $X$, ie the first weight vector $w_1$ solves: $$\\max_{\\lvert\\lvert w\\rvert \\rvert} \\sum_i (t_{1,i})^2.$$ Other weights are retrieved by the same operation on $X$ minus its first components.\n",
        "\n",
        "This process yields a decomposition of the form $T=XW$. Using this: \n",
        "- dimensionality reduction can be performed in this context by only selecting the first $L$ components: $T_L=X W_L$;\n",
        "- a first idea for visualization in $\\mathbb{R}^2$ is to only represent the first two components.\n",
        "\n",
        "Let us try this out on the MNIST dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4diF0AWKfHD",
        "colab_type": "text"
      },
      "source": [
        "### 4) Perform PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ezEaoIDZM9Ax",
        "colab": {}
      },
      "source": [
        "# Perform PCA\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QEapNy0GN5rd",
        "colab": {}
      },
      "source": [
        "chart = ggplot( df.loc[rndperm[:3000],:], aes(x='pca-one', y='pca-two', color='label') ) \\\n",
        "        + geom_point(size=75,alpha=0.8) \\\n",
        "        + ggtitle(\"First and Second Principal Components colored by digit\")\n",
        "chart"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeTN1QNMeD_L",
        "colab_type": "text"
      },
      "source": [
        "### 5) Make inference about this method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP8KwZ1rLbrS",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6Qy7KKJeD_M",
        "colab_type": "text"
      },
      "source": [
        "### Dimensionality Reduction through t-distributed stochastic neighbor embedding (t-SNE)\n",
        "\n",
        "On the other hand, we study t-SNE, which is a nonlinear dimensionality reduction method particularly suited for the visualization of high-dimensional data.\n",
        "\n",
        "The idea is reduce dimensionality by minimizing the distance between pairwise probabilities in the high-dimensional space and in a lower-dimensional space. It consists of two steps:\n",
        "- constructing a probability on the high dimensional object pairs: similar pairs have a high probability of being chosen, unlike dissimilar observations;\n",
        "- creating a similar probability distribution in a low dimensional space, minimizing some distance between the two.\n",
        "\n",
        "Let us try it: is it possible to apply it on all the data? Does that improve the visual performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiFU2w8NLGvC",
        "colab_type": "text"
      },
      "source": [
        "### 6) Use t-SNE model to do dimensionality reduction and plot results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mje1Ldg9OtrV",
        "colab": {}
      },
      "source": [
        "# T-sne\n",
        "n_sne = 7000 # data subsampling"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92QQM9CRLXt0",
        "colab_type": "text"
      },
      "source": [
        "### 7) Make inference about this method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tb8E5sReD_W",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmCH4ybfeD_Y",
        "colab_type": "text"
      },
      "source": [
        "### Combination of the two.\n",
        "\n",
        "Since PCA handles large inputs better, a solution would be to combine the two, and perform t-SNE on PCA outputs. How does that improve performance ? How far can we go in terms of components ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfuqs1DxR-bC",
        "colab_type": "text"
      },
      "source": [
        "### 8) use 50 dimentions with PCA model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PQG7B3XdS9On",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCufXEGGSIlK",
        "colab_type": "text"
      },
      "source": [
        "### 9) Implement the tSNE model and visualise results and make inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GtXUL5uGTRP7",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}