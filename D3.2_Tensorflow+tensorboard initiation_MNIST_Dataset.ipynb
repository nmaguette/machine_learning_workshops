{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"D3.2_Tensorflow+tensorboard initiation_MNIST_Dataset.ipynb","provenance":[{"file_id":"1uzQQoPDQld1Th5WAPZWkTY6qKs0R2aOC","timestamp":1572527895000}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Zu16Psl6Bmzc","colab_type":"code","outputId":"5096562d-e9ac-4044-cb24-4b2a660eaf2a","executionInfo":{"status":"ok","timestamp":1572887983929,"user_tz":-60,"elapsed":2527,"user":{"displayName":"Ndèye Maguette MBAYE","photoUrl":"","userId":"01718406775529037435"}},"colab":{"base_uri":"https://localhost:8080/","height":63}},"source":["from numpy.random import seed\n","seed(42)\n","from tensorflow import set_random_seed\n","set_random_seed(42)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"NtMdAihRBnzu","colab_type":"code","colab":{}},"source":["import os\n","import numpy as np\n","import tensorflow as tf\n","\n","from time import strftime\n","from PIL import Image"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YueLFzrlCRIz","colab_type":"code","colab":{}},"source":["NR_CLASSES = 10\n","VALIDATION_SIZE = 10000\n","IMAGE_WIDTH = 28\n","IMAGE_HEIGHT = 28\n","CHANNELS = 1\n","TOTAL_INPUTS = IMAGE_WIDTH*IMAGE_HEIGHT*CHANNELS"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qwDC3cvHDkqz","colab_type":"text"},"source":["### 1) Download mnist dataset and create train and test sets."]},{"cell_type":"code","metadata":{"id":"h0XFWCtzBtZD","colab_type":"code","outputId":"f0fdcabe-7353-4028-db6d-ec5e9eedc70f","executionInfo":{"status":"ok","timestamp":1572887985356,"user_tz":-60,"elapsed":3924,"user":{"displayName":"Ndèye Maguette MBAYE","photoUrl":"","userId":"01718406775529037435"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["from keras.datasets import mnist\n","\n","(X_train_all, y_train_all), (X_test, y_test) = mnist.load_data()\n","\n","num_classes = np.unique(y_train_all).shape[0]\n","print(\"Shape of training dataset:\", X_train_all.shape)\n","print(\"Number of training examples:\", X_train_all.shape[0])\n","print(\"Number of testing examples:\", X_test.shape[0])\n","print(\"Number of classes:\", num_classes)\n","print(\"Image shape:\", X_train_all[0].shape)\n","print(\"Image data type:\", X_train_all.dtype)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n","11493376/11490434 [==============================] - 1s 0us/step\n","Shape of training dataset: (60000, 28, 28)\n","Number of training examples: 60000\n","Number of testing examples: 10000\n","Number of classes: 10\n","Image shape: (28, 28)\n","Image data type: uint8\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xjZr4NWFD_9M","colab_type":"text"},"source":["### 2) Reshape and rescale data: make it between 0 and 1 - neural networs usually work better with this type of data"]},{"cell_type":"code","metadata":{"id":"bu__ahAsCADX","colab_type":"code","outputId":"6a3ca78c-1f4a-424d-ebd4-f2b51f3f25c9","executionInfo":{"status":"ok","timestamp":1572887986313,"user_tz":-60,"elapsed":4835,"user":{"displayName":"Ndèye Maguette MBAYE","photoUrl":"","userId":"01718406775529037435"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["from sklearn.preprocessing import normalize\n","\n","\n","# Reshape the data from 3 dimensions to 2\n","X_train_all = X_train_all.reshape((X_train_all.shape[0],TOTAL_INPUTS))\n","X_test = X_test.reshape((X_test.shape[0],TOTAL_INPUTS))\n","# Re-scale the train and test set\n","X_train_all = X_train_all/255.0\n","X_test = X_test/ 255.0\n","\n","y_train_all = np.eye(NR_CLASSES)[y_train_all]\n","y_test = np.eye(NR_CLASSES)[y_test]\n","\n","print(y_train_all.shape)\n","print(y_test.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(60000, 10)\n","(10000, 10)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UqdkZKoMDc-N","colab_type":"text"},"source":["### 3) Split the training dataset into a smaller training dataset and a validation dataset for the features and the labels. Create four arrays: x_val, y_val, x_train, and y_train from x_train_all and y_train_all. Use the validation size of 10,000."]},{"cell_type":"code","metadata":{"id":"XxVE0kk7G0-N","colab_type":"code","outputId":"a967cf8c-4a33-4aa3-8b7d-2ae6c13fb981","executionInfo":{"status":"ok","timestamp":1572887986315,"user_tz":-60,"elapsed":4823,"user":{"displayName":"Ndèye Maguette MBAYE","photoUrl":"","userId":"01718406775529037435"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["X_val = X_train_all[:VALIDATION_SIZE]\n","y_val = y_train_all[:VALIDATION_SIZE]\n","\n","X_train = X_train_all[VALIDATION_SIZE:]\n","y_train = y_train_all[VALIDATION_SIZE:]\n","\n","print(\"Training set shape:\", X_train.shape)\n","print(\"Validation set shape:\", X_val.shape)\n","print(\"Testing set shape:\", X_test.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Training set shape: (50000, 784)\n","Validation set shape: (10000, 784)\n","Testing set shape: (10000, 784)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R0vPUT-sI6mO","colab_type":"code","outputId":"3cecb783-8789-4edd-c927-636ba282ecbd","executionInfo":{"status":"ok","timestamp":1572887986316,"user_tz":-60,"elapsed":4809,"user":{"displayName":"Ndèye Maguette MBAYE","photoUrl":"","userId":"01718406775529037435"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["print(\"Training set shape:\", y_train.shape)\n","print(\"Validation set shape:\", y_val.shape)\n","print(\"Testing set shape:\", y_test.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Training set shape: (50000, 10)\n","Validation set shape: (10000, 10)\n","Testing set shape: (10000, 10)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"w-043Q2iEyFE","colab_type":"text"},"source":["### 4) Setup Tensorflow Graph"]},{"cell_type":"code","metadata":{"id":"lt3B0Hna8PWd","colab_type":"code","colab":{}},"source":["#tf.reset_default_graph()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"leDq78OMEdZV","colab_type":"code","outputId":"0912a7a9-d2a6-47ab-caf3-5e0d14859845","executionInfo":{"status":"ok","timestamp":1572887986318,"user_tz":-60,"elapsed":4792,"user":{"displayName":"Ndèye Maguette MBAYE","photoUrl":"","userId":"01718406775529037435"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["X = tf.placeholder(tf.float32, shape=[None, TOTAL_INPUTS], name='X')\n","Y = tf.placeholder(tf.float32, shape=[None, NR_CLASSES], name='labels')\n","\n","print(X)\n","print(Y)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Tensor(\"X:0\", shape=(?, 784), dtype=float32)\n","Tensor(\"labels:0\", shape=(?, 10), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VZBtcVXBFCvp","colab_type":"text"},"source":["### 5) Create variables for number of epochs, learning rate and two hidden layers: 512 and 64 neurons"]},{"cell_type":"code","metadata":{"id":"foCD_-ejEwjr","colab_type":"code","colab":{}},"source":["nr_epochs = 16\n","learning_rate = 0.01\n","\n","n_hidden1 = 512\n","n_hidden2 = 64"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9jzIC73bG4pG","colab_type":"text"},"source":["# Setup of tensorboard on google colab\n","Tensorboard is the very good way to visualise your data. "]},{"cell_type":"code","metadata":{"id":"KagdMxH4GNAH","colab_type":"code","outputId":"0875b7da-0562-4dfb-c4f3-a1264fc819c9","executionInfo":{"status":"ok","timestamp":1572887990475,"user_tz":-60,"elapsed":8933,"user":{"displayName":"Ndèye Maguette MBAYE","photoUrl":"","userId":"01718406775529037435"}},"colab":{"base_uri":"https://localhost:8080/","height":255}},"source":["!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","!unzip ngrok-stable-linux-amd64.zip\n","\n","LOG_DIR = './log'\n","get_ipython().system_raw(\n","    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n","    .format(LOG_DIR)\n",")\n","\n","get_ipython().system_raw('./ngrok http 6006 &')\n","\n","! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2019-11-04 17:19:47--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","Resolving bin.equinox.io (bin.equinox.io)... 3.214.169.236, 35.170.171.200, 52.7.202.148, ...\n","Connecting to bin.equinox.io (bin.equinox.io)|3.214.169.236|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 13773305 (13M) [application/octet-stream]\n","Saving to: ‘ngrok-stable-linux-amd64.zip’\n","\n","ngrok-stable-linux- 100%[===================>]  13.13M  17.7MB/s    in 0.7s    \n","\n","2019-11-04 17:19:48 (17.7 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13773305/13773305]\n","\n","Archive:  ngrok-stable-linux-amd64.zip\n","  inflating: ngrok                   \n","http://9022b868.ngrok.io\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xF-KGlt8FMO-","colab_type":"text"},"source":["### 6) Create function to proceed one layer in neural network:\n","You have input, dimension of weight: weight_dim, dimension of bias: bias_dim and name of your layer, return the output layer.\n","Use truncated normal distribution to generate initial weights and zero-constants for biases. If the name = \"out\", use softmax, in other cas use relu"]},{"cell_type":"code","metadata":{"id":"p-3yRrdgFRA8","colab_type":"code","colab":{}},"source":["def setup_layer(input, weight_dim, bias_dim, name):\n","    \n","    with tf.name_scope(name):\n","        initial_w = tf.truncated_normal(weight_dim, stddev=0.1, seed=42)  \n","        w = tf.Variable(initial_value=initial_w)  \n","        \n","        initial_b = tf.constant(value=0.0, shape=bias_dim)\n","        b = tf.Variable(initial_value=initial_b) \n","        \n","        layer_in = tf.matmul(input, w) + b\n","        if name =='out':\n","          layer_out = tf.nn.softmax(layer_in)\n","        else :\n","          layer_out = tf.nn.relu(layer_in)\n","\n","        tf.summary.histogram('weights', w)\n","        tf.summary.histogram('biases', b)\n","        \n","        return layer_out"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KWMKGrW4Fx6G","colab_type":"text"},"source":["### 7) Create neural network with 2 hidden layers, using this function from previous item. Add also one dropout layer to avoid overfitting"]},{"cell_type":"code","metadata":{"id":"lesgVnOkFvM3","colab_type":"code","outputId":"f141b19d-5927-44bd-f482-107f69d0b3f5","executionInfo":{"status":"ok","timestamp":1572887990637,"user_tz":-60,"elapsed":9078,"user":{"displayName":"Ndèye Maguette MBAYE","photoUrl":"","userId":"01718406775529037435"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["layer_1 = setup_layer(X, [TOTAL_INPUTS, n_hidden1], [n_hidden1], 'layer1')\n","layer_drop = tf.nn.dropout(layer_1, rate=0.2)\n","layer_2 = setup_layer(layer_drop, [n_hidden1, n_hidden2], [n_hidden2], 'layer2')\n","output = setup_layer(layer_2, [n_hidden2, NR_CLASSES], [NR_CLASSES], 'out')\n","\n","model_name = f'{n_hidden1}-DO-{n_hidden2} LR{learning_rate} E{nr_epochs}'\n","\n","model_name"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'512-DO-64 LR0.01 E16'"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"5RYxCatJNqZQ","colab_type":"text"},"source":["### 8) For better visualization in TensorBoard we want to use tf.name_scope() to aggregate loss, optimizer, accuracy metrica and performance."]},{"cell_type":"code","metadata":{"id":"ejuDsIObGgyH","colab_type":"code","colab":{}},"source":["# Defining Loss Function\n","with tf.name_scope('loss_calc'):\n","    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(Y, output))\n","# Defining Optimizer\n","with tf.name_scope('optimizer'):\n","    optimizer = tf.train.AdamOptimizer(learning_rate)\n","    train_step = optimizer.minimize(loss)\n","# Accuracy Metric\n","with tf.name_scope('accuracy_calc'):\n","    correct_pred = tf.cast(tf.equal(tf.argmax(output, 1), tf.argmax(Y, 1)), tf.float32)\n","    accuracy = tf.reduce_mean(correct_pred)\n","\n","# Add summaries for tensorboard\n","with tf.name_scope('performance'):\n","    tf.summary.scalar('accuracy', accuracy)\n","    tf.summary.scalar('cost', loss)\n","\n","#Add more summaries for tensorboard: check Input Images in Tensorboard\n","with tf.name_scope('show_image'):\n","    x_image = tf.reshape(X, [-1, 28, 28, 1])\n","    tf.summary.image('image_input', x_image, max_outputs=4)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jxxIfeCkOYmb","colab_type":"text"},"source":["### 9) Create session using tf.Session(), merge summaries using tf.summary.merge_all(). Use tf.summary.FileWriter() to write you summaries."]},{"cell_type":"code","metadata":{"id":"G0_Kp5PqHZyn","colab_type":"code","colab":{}},"source":["#Run Session\n","\n","sess = tf.Session()\n","\n","merged_summary = tf.summary.merge_all()\n","\n","train_writer = tf.summary.FileWriter(LOG_DIR+'/train')\n","train_writer.add_graph(sess.graph)\n","\n","validation_writer = tf.summary.FileWriter(LOG_DIR+'/val')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HCOWynWYPEL4","colab_type":"text"},"source":["### 10) Initialise all the variables, and run the session, look at the TensorBoard"]},{"cell_type":"code","metadata":{"id":"fcc-SvdCIvev","colab_type":"code","colab":{}},"source":["init = tf.global_variables_initializer()\n","sess.run(init)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fs2U22flPSS4","colab_type":"text"},"source":["### 11) If you data is quite big, it is usefull to have so-called batches, smaller pieces of data. We have 50000 data points, we want to have batches with 1000 points. Create next_batch function, which gives you the next part of the data"]},{"cell_type":"code","metadata":{"id":"4VqiQFnmRXKg","colab_type":"code","colab":{}},"source":["size_of_batch = 1000\n","num_examples = X_train.shape[0]\n","nr_iterations = int(num_examples/size_of_batch)\n","\n","index_in_epoch = 0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MaQE1pAJJIxE","colab_type":"code","colab":{}},"source":["def next_batch(batch_size, data, labels):\n","    \n","    global num_examples\n","    global index_in_epoch\n","\n","    start = index_in_epoch\n","    index_in_epoch += batch_size\n","    if index_in_epoch > num_examples:\n","        start = 0\n","        index_in_epoch = batch_size\n","    end = index_in_epoch\n","    return data[start:end], labels[start:end]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VMACFD2yPh7w","colab_type":"text"},"source":["### 12) Run the algorighm: do several so-called epochs - the runs through all the data. In each epoch use 50 batches with 1000 data points. Write information to TensorBoard to investigate later."]},{"cell_type":"code","metadata":{"id":"3DLPdUMvx-ZL","colab_type":"code","outputId":"f4bf645a-b833-46c2-caed-2af8ba188166","executionInfo":{"status":"ok","timestamp":1572888043958,"user_tz":-60,"elapsed":62325,"user":{"displayName":"Ndèye Maguette MBAYE","photoUrl":"","userId":"01718406775529037435"}},"colab":{"base_uri":"https://localhost:8080/","height":578}},"source":["nr_epochs = 16\n","\n","for epoch in range(nr_epochs):\n","    \n","    # ============= Training Dataset ============\n","    for i in range(nr_iterations):\n","        batch_x, batch_y = next_batch(size_of_batch, X_train, y_train)\n","\n","        feed_dictionary = {X : batch_x, Y : batch_y}\n","\n","        sess.run(train_step, feed_dict=feed_dictionary)\n","    \n","    train_acc, train_merged_summary = sess.run([accuracy, merged_summary],\n","                                               feed_dict=feed_dictionary)\n","    \n","    # write summary: merged_summary and accuracy to the TensorBoard\n","    train_writer.add_summary(train_merged_summary, epoch)\n","    \n","    print(f'Epoch {epoch} \\t| Training Accuracy = {train_acc}')\n","    \n","    # ========== Validation Dataset =============\n","    val_acc, val_merged_summary = sess.run([accuracy, merged_summary],\n","                                           feed_dict={X: X_val, Y: y_val})\n","    #add summary for validation data\n","    validation_writer.add_summary(val_merged_summary, epoch)\n","    print(f'Epoch {epoch} \\t| Validation Accuracy = {val_acc}')\n","\n","print('Done training!')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 0 \t| Training Accuracy = 0.7839999794960022\n","Epoch 0 \t| Validation Accuracy = 0.7651000022888184\n","Epoch 1 \t| Training Accuracy = 0.9570000171661377\n","Epoch 1 \t| Validation Accuracy = 0.9406999945640564\n","Epoch 2 \t| Training Accuracy = 0.9729999899864197\n","Epoch 2 \t| Validation Accuracy = 0.953499972820282\n","Epoch 3 \t| Training Accuracy = 0.9729999899864197\n","Epoch 3 \t| Validation Accuracy = 0.9538999795913696\n","Epoch 4 \t| Training Accuracy = 0.9710000157356262\n","Epoch 4 \t| Validation Accuracy = 0.9595999717712402\n","Epoch 5 \t| Training Accuracy = 0.9739999771118164\n","Epoch 5 \t| Validation Accuracy = 0.9585999846458435\n","Epoch 6 \t| Training Accuracy = 0.9800000190734863\n","Epoch 6 \t| Validation Accuracy = 0.9598000049591064\n","Epoch 7 \t| Training Accuracy = 0.9750000238418579\n","Epoch 7 \t| Validation Accuracy = 0.9627000093460083\n","Epoch 8 \t| Training Accuracy = 0.9819999933242798\n","Epoch 8 \t| Validation Accuracy = 0.9623000025749207\n","Epoch 9 \t| Training Accuracy = 0.9739999771118164\n","Epoch 9 \t| Validation Accuracy = 0.9603999853134155\n","Epoch 10 \t| Training Accuracy = 0.9819999933242798\n","Epoch 10 \t| Validation Accuracy = 0.9652000069618225\n","Epoch 11 \t| Training Accuracy = 0.9779999852180481\n","Epoch 11 \t| Validation Accuracy = 0.961899995803833\n","Epoch 12 \t| Training Accuracy = 0.9779999852180481\n","Epoch 12 \t| Validation Accuracy = 0.9621000289916992\n","Epoch 13 \t| Training Accuracy = 0.9789999723434448\n","Epoch 13 \t| Validation Accuracy = 0.9621999859809875\n","Epoch 14 \t| Training Accuracy = 0.9810000061988831\n","Epoch 14 \t| Validation Accuracy = 0.9702000021934509\n","Epoch 15 \t| Training Accuracy = 0.9750000238418579\n","Epoch 15 \t| Validation Accuracy = 0.9617999792098999\n","Done training!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2kTrt30KSRz-","colab_type":"text"},"source":["### 13) Calculate the accuracy over the test dataset (x_test and y_test). Use your knowledge of running a session to get the accuracy. Display the accuracy as a percentage rounded to two decimal numbers."]},{"cell_type":"code","metadata":{"id":"g-oNL6CWLXT5","colab_type":"code","outputId":"be6ea116-9925-479f-ab92-d11bdedcdcda","executionInfo":{"status":"ok","timestamp":1572888065763,"user_tz":-60,"elapsed":1136,"user":{"displayName":"Ndèye Maguette MBAYE","photoUrl":"","userId":"01718406775529037435"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["sess.run(train_step, feed_dict={X: X_test, Y: y_test})\n","test_accuracy = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n","\n","print(\"Testing Accuracy: {0:.2f}\".format(test_accuracy*100))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Testing Accuracy: 96.25\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nYZVjiVZSYD3","colab_type":"text"},"source":["14) IMPORATANT: Reset for the Next Run"]},{"cell_type":"code","metadata":{"id":"WjqawdXTJMLJ","colab_type":"code","colab":{}},"source":["# Reset for the Next Run\n","\n","train_writer.close()\n","validation_writer.close()\n","sess.close()\n","tf.reset_default_graph()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vgd0-nF3bHqq","colab_type":"text"},"source":["# Keras Implementation\n","\n","Let's now dive into the implementation of our first neural network.\n","Our network is a simple neural network, **without convolution operations**.\n","\n","We make use of the **sequential paradigm** of Tensorflow, made to build models by plugging together building blocks. This interface allows for easier code writing, while Tensorflow also offers alternative ways to write more complex deep learning algorithms through the use of its **define-by-run interface**.\n","\n","The network's structure is the following :\n"," - A **flatten** layer, used to vectorize the whole input batch of data\n"," - A **dense** layer, transforming the 28x28=784 input data to a 512 vector, using a rectified linear unit activation function\n"," - A **dropout** layer, ensuring the network does not overfit the training data by giving each of its neuron a 20% chance not to be activated at each stage\n"," - A **dense** layer, outputing a 10 vector using a softmax function\n","\n","The optimizer we use at first is named **Adam**, because it requires very little parameter tuning.\n","\n","We use the sparse categorical crossentropy loss function because each sample of our data belongs to exactly one class (i.e. each handwritten digit represents only one specific digit).\n","\n","We also use the **accuracy** metric, which is basically the percentage of correct predictions our network computes.\n","\n","We will then train this neural network for **5 epochs** (i.e. on the whole dataset five times), and then test it on the testing set"]},{"cell_type":"code","metadata":{"id":"Hk3HwcVZHqaE","colab_type":"code","colab":{}},"source":["from tensorflow.keras.layers import Dense, Flatten, Dropout\n","from tensorflow.keras.losses import categorical_crossentropy\n","from tensorflow.keras.optimizers import Adam"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XYUYZFBabH-E","colab_type":"code","outputId":"6ce5dd7b-4366-4d4d-c6f5-508897bbf349","executionInfo":{"status":"ok","timestamp":1572889188199,"user_tz":-60,"elapsed":441,"user":{"displayName":"Ndèye Maguette MBAYE","photoUrl":"","userId":"01718406775529037435"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["# Let's implement the network first\n","model = tf.keras.models.Sequential([Flatten(input_shape=X_train.shape[1:]),   # Flatten layer\n","                                    Dense(n_hidden1, activation='relu'),      # Dense layer\n","                                    Dropout(0.2),                             # Dropout layer\n","                                    Dense(NR_CLASSES, activation='softmax')   # Dense layer\n","                                  ])\n","\n","# Then choose the optimizer, loss function, and metric, as compilation parameters\n","model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ggwKe-BfJH9x","colab_type":"code","outputId":"08ffa1da-933b-4041-fa0f-105e14be44d8","executionInfo":{"status":"ok","timestamp":1572889246635,"user_tz":-60,"elapsed":50612,"user":{"displayName":"Ndèye Maguette MBAYE","photoUrl":"","userId":"01718406775529037435"}},"colab":{"base_uri":"https://localhost:8080/","height":221}},"source":["# Train the model we just built\n","model.fit(X_train, y_train, validation_data=(X_val, y_val),\n","          epochs=5, verbose=2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 50000 samples, validate on 10000 samples\n","Epoch 1/5\n","50000/50000 - 11s - loss: 0.2368 - acc: 0.9289 - val_loss: 0.1244 - val_acc: 0.9624\n","Epoch 2/5\n","50000/50000 - 11s - loss: 0.1039 - acc: 0.9681 - val_loss: 0.0935 - val_acc: 0.9696\n","Epoch 3/5\n","50000/50000 - 11s - loss: 0.0726 - acc: 0.9778 - val_loss: 0.0829 - val_acc: 0.9748\n","Epoch 4/5\n","50000/50000 - 9s - loss: 0.0567 - acc: 0.9812 - val_loss: 0.0796 - val_acc: 0.9762\n","Epoch 5/5\n","50000/50000 - 9s - loss: 0.0448 - acc: 0.9860 - val_loss: 0.0721 - val_acc: 0.9773\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fdab52abc50>"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"5SO6FIfjc7lY","colab_type":"code","outputId":"d76e96fa-0f57-4495-88a3-a6e466e43e86","executionInfo":{"status":"ok","timestamp":1572889401387,"user_tz":-60,"elapsed":1350,"user":{"displayName":"Ndèye Maguette MBAYE","photoUrl":"","userId":"01718406775529037435"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# And evaluate its performances on the testing set\n","pred_loss, pred_acc = model.evaluate(X_test, y_test)\n","\n","# Now finally print the value of the loss and metric functions specified above\n","print(\"The loss on the test set is:\", pred_loss)\n","print(\"The accuracy on the test set is:\", pred_acc)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["10000/10000 [==============================] - 1s 97us/sample - loss: 0.0745 - acc: 0.9789\n","The loss on the test set is: 0.0744911439513322\n","The accuracy on the test set is: 0.9789\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mxJ69aFZc73S","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}